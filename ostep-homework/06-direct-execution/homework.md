## Question
In this homework, you’ll measure the costs of a system call and context switch. Measuring the cost of a system call is relatively easy. For example, you could repeatedly call a simple system call (e.g., performing a 0-byte read), and time how long it takes; dividing the time by the number of iterations gives you an estimate of the cost of a system call.

One thing you’ll have to take into account is the precision and accuracy of your timer. A typical timer that you can use is `gettimeofday()`; read the man page for details. What you’ll see there is that `gettimeofday()` returns the time in microseconds since 1970; however, this does not mean that the timer is precise to the microsecond. Measure back-to-back calls `togettimeofday()` to learn something about how precise the timer really is; this will tell you how many iterations of your null system-call test you’ll have to run in order to get a good measurement result.  If `gettimeofday()` is not precise enough for you, you might look into using the `rdtsc` instruction available on x86 machines.

Measuring the cost of a context switch is a little trickier. The `lmbench` benchmark does so by running two processes on a single CPU, and setting up two UNIXpipes between them; a pipe is just one of many ways processes in a UNIX system can communicate with one another. The first process then issues a write to the first pipe, and waits for a read on the second; upon seeing the first process waiting for something to read from the second pipe, the OS puts the first process in the blocked state, and switches to the other process, which reads from the first pipe and then writes to the second. When the second process tries to read from the first pipe again, it blocks, and thus the back-and-forth cycle of communication continues. By measuring the cost of communicating like this repeatedly, `lmbench` can make a good estimate of the cost of a context switch. You can try to recreate something similar here, using pipes, or perhaps some other communication mechanism such as UNIXsockets.

One difficulty in measuring context-switch cost arises in systems with more than one CPU; what you need to do on such a system is ensure that your context-switching processes are located on the same processor. Fortunately, most operating systems have calls to bind a process to a particular processor; on Linux, for example, the `sched_setaffinity()` call is what you’re looking for. By ensuring both processes are on the same processor, you are making sure to measure the cost of the OS stopping one process and restoring another on the same CPU.

## Notes
### Getting the timing of system call
This first task is relatively easy, at least conceptually. I need to start a clock, make a system call (the question suggests a 0 byte read) and then stop the clock, then print the timings.

The first thing that I notice is that there seem to be dozens of ways to get and play with time. I found [this answer](https://stackoverflow.com/questions/12392278/measure-time-in-linux-time-vs-clock-vs-getrusage-vs-clock-gettime-vs-gettimeof) really helpful. I tried a few methods, first with `clock()` then with `clock_gettime()`. The latter is recommended over `gettimeofday()`, so it is what I am going with. Seems to have a decent precision. There are some other options [here](https://levelup.gitconnected.com/8-ways-to-measure-execution-time-in-c-c-48634458d0f9) as well. Time is a tricky concept in computing, so it is no surprise that in 50 years of Cs existence there are a few different ways to slice the cat.

I also wonder what the overhead of the `for` loop is. I could probably take a gander at the assembly code that is generated, count up instructions for a for loop (outside of the system call), multiply that by count of iterations, and subtract to get a true sense of what the system call costs.

Another interesting thing is that apparently there is signifcant difference in the overhead to call `clock()` and `clock_gettime()`. Running a system call for the former takes 3 μs and the latter takes 700 or 800 μs. These are such infinitesimally small numbers that I can't imagine the use cases were this precision would be needed, but it makes it hard to understand the true cost of a system call. I'd defer to the 3 μs to make a simple system call, minus the overhead of clock.

One other thing that is interesting is the construction of the system call: `read(0, NULL, 0);`. `read()` takes a file descriptor as its first argument, and a file descriptor is just an integer to refer to an open file. There are a few reserved file descriptors, 0 --> stdin, 1 --> stdout, 2 --> stderr. These are opened automatically when any process starts.

### Measuring the cost of a context switch
A good summary [here](https://stackoverflow.com/questions/69118118/how-to-measure-the-cost-of-context-switching-more-precisely)
